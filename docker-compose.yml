networks:
  netw:
    driver: bridge

volumes:
  namenode_data:
  datanode_data:
  hive_metastore_db:
  mart_db:
  airflow_db:

services:
  # ---------------------------
  # Kafka
  # ---------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks: [netw]
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports: ["2181:2181"]

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    networks: [netw]
    depends_on: [zookeeper]
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_MAX_REQUEST_SIZE: 500000000
      KAFKA_MESSAGE_MAX_BYTES: 500000000

  # ---------------------------
  # HDFS
  # ---------------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.3-java8
    container_name: namenode
    hostname: namenode
    networks: [netw]
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
      CLUSTER_NAME: "test"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.3-java8
    container_name: datanode
    hostname: datanode
    networks: [netw]
    depends_on: [namenode]
    ports:
      - "9864:9864"
      - "9866:9866"
    volumes:
      - datanode_data:/hadoop/dfs/data
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"

  hdfs-init:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.3-java8
    container_name: hdfs-init
    networks: [netw]
    depends_on:
      - namenode
      - datanode
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
    entrypoint: ["/bin/bash","-lc"]
    command: |
      set -euo pipefail
      H=/opt/hadoop-3.1.3/bin/hdfs

      echo "Waiting for HDFS..."
      for i in $(seq 1 60); do
        if $H dfs -fs hdfs://namenode:8020 -ls / >/dev/null 2>&1; then
          break
        fi
        sleep 2
      done

      echo "Creating HDFS directories..."
      $H dfs -fs hdfs://namenode:8020 -mkdir -p \
        /tmp \
        /user/hive/warehouse \
        /datalake/bronze/amazon_sales \
        /datalake/silver/amazon_sales \
        /datalake/gold/amazon_sales_aggregated \
        /checkpoints

      $H dfs -fs hdfs://namenode:8020 -chmod -R 777 \
        /tmp /user/hive/warehouse /datalake /checkpoints

      echo "âœ… HDFS init done"
      $H dfs -fs hdfs://namenode:8020 -ls /datalake
    restart: "no"

  # ---------------------------
  # Hive Metastore
  # ---------------------------
  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    networks: [netw]
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    hostname: hive-metastore
    networks: [netw]
    depends_on: [hive-metastore-postgresql, namenode, datanode, hdfs-init]
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql:5432/metastore"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionUserName: "hive"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionPassword: "hive"
    volumes:
      - ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/hive-site.xml:/etc/hadoop/hive-site.xml
    command: ["/opt/hive/bin/hive", "--service", "metastore"]
    ports: ["9083:9083"]

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    hostname: hive-server
    networks: [netw]
    depends_on: [hive-metastore]
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore-postgresql:5432/metastore"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionUserName: "hive"
      HIVE_CORE_CONF_javax_jdo_option_ConnectionPassword: "hive"
    volumes:
      - ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/hive-site.xml:/etc/hadoop/hive-site.xml
      - ./hive/create_hive_tables.sql:/opt/hive/create_hive_tables.sql
    entrypoint: ["bash", "-lc"]
    command: >
      /opt/hive/bin/hiveserver2 &
      echo "Waiting HiveServer2...";
      for i in $(seq 1 60); do
        if (echo > /dev/tcp/localhost/10000) >/dev/null 2>&1; then break; fi;
        sleep 2;
      done;
      echo "Init Hive tables...";
      /opt/hive/bin/beeline -u jdbc:hive2://localhost:10000 -n hive -f /opt/hive/create_hive_tables.sql || true;
      wait
    ports: ["10000:10000", "10002:10002"]

  # ---------------------------
  # Spark
  # ---------------------------
  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2-java11
    container_name: spark-master
    hostname: spark-master
    networks: [netw]
    depends_on: [namenode, datanode]
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      INIT_DAEMON_STEP: "false"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
    volumes:
      - ./:/app

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2-java11
    container_name: spark-worker-1
    hostname: spark-worker-1
    networks: [netw]
    depends_on: [spark-master]
    ports: ["8081:8081"]
    environment:
      SPARK_MASTER: "spark://spark-master:7077"

  # ---------------------------
  # Postgres Mart (OLTP + OLAP)
  # ---------------------------
  postgres-mart:
    image: postgres:15
    container_name: postgres-mart
    networks: [netw]
    ports: ["5433:5432"]
    environment:
      POSTGRES_USER: mart
      POSTGRES_PASSWORD: mart
      POSTGRES_DB: martdb
    volumes:
      - mart_db:/var/lib/postgresql/data
      - ./postgres/init_mart.sql:/docker-entrypoint-initdb.d/init_mart.sql

  # ---------------------------
  # Producer
  # ---------------------------
  producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: producer-amazon
    networks: [netw]
    depends_on: [kafka]
    environment:
      KAFKA_BROKER: "kafka:9092"
      KAFKA_TOPIC: "${KAFKA_TOPIC:-amazon-sales}"
      CSV_FILE: "${CSV_FILE:-/data/Amazon.csv}"
      RATE: "${RATE:-200}"
    volumes:
      - ./:/app
      - ./data:/data
    working_dir: /app
    command: ["python", "producer/kafka_producer_amazon.py"]

  # ---------------------------
  # Spark Streaming (Kafka -> HDFS + Postgres)
  # ---------------------------
  spark-streaming:
    image: bde2020/spark-master:3.0.0-hadoop3.2-java11
    container_name: spark-streaming
    networks: [netw]
    depends_on: [kafka, spark-master, hdfs-init, postgres-mart]
    volumes:
      - ./:/app
    working_dir: /app
    environment:
      KAFKA_BROKER: "kafka:9092"
      KAFKA_TOPIC: "${KAFKA_TOPIC:-amazon-sales}"
      HDFS_URL: "hdfs://namenode:8020"
      PG_URL: "jdbc:postgresql://postgres-mart:5432/martdb"
      PG_USER: "mart"
      PG_PASS: "mart"
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        echo "Waiting 25s for services..."
        sleep 25

        echo "Launching Spark Streaming job..."
        /spark/bin/spark-submit \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.postgresql:postgresql:42.7.3 \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          /app/spark/spark_streaming_kafka_to_lakehouse_and_postgres.py
  # ---------------------------
  # Streamlit App (OLTP)
  # ---------------------------
  streamlit:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: streamlit-oltp
    networks: [netw]
    depends_on: [postgres-mart]
    ports: ["8502:8501"]

    environment:
      PG_HOST: "postgres-mart"
      PG_PORT: "5432"
      PG_DB: "martdb"
      PG_USER: "mart"
      PG_PASSWORD: "mart"

  # ---------------------------
  # Airflow
  # ---------------------------
  airflow-db:
    image: postgres:15
    container_name: airflow-db
    networks: [netw]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    networks: [netw]
    depends_on: [airflow-db, postgres-mart, spark-master]
    ports: ["8088:8080"]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: "devsecret"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    entrypoint: ["bash", "-lc"]
    command: >
      airflow db init &&
      airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com || true &&
      airflow webserver

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    networks: [netw]
    depends_on: [airflow-db, airflow-webserver]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: ["bash", "-lc", "airflow scheduler"]
